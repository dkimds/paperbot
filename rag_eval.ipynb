{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Creating a dataset with questions and their expected answers(Omitted)\n",
    "2. Running your RAG application on those questions\n",
    "3. Using evaluators to measure how well your application performed, looking at factors like:\n",
    "    - Answer relevance\n",
    "    - Answer accuracy\n",
    "    - Retrieval quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./temp.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "### ëŒ€í˜•ì–¸ì–´ëª¨ë¸, ë©”ëª¨ë¦¬, íŒŒì„œ ì„¤ì • ###\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "### ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ###\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ë¦¬íŠ¸ë¦¬ë²„íˆ´ êµ¬ì¶• ###\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"paper_retriever\",\n",
    "    \"Search for information about a paper. For any questions about the paper, you must use this tool!\",\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "# Add decorator so this function is traced in LangSmith\n",
    "@traceable()\n",
    "def rag_bot(question: str) -> dict:\n",
    "    # langchain ChatModel will be automatically traced\n",
    "    ai_msg = agent_executor.invoke({\"input\": question})\n",
    "\n",
    "    return {\"answer\": ai_msg[\"output\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define the examples for the dataset\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"What is SAM 2's primary capability?\"},\n",
    "    \"outputs\": {\"answer\": \"Promptable visual segmentation in both images and videos\"}\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"How much faster is SAM 2 than SAM for image segmentation?\"},\n",
    "    \"outputs\": {\"answer\": \"6Ã— faster while being more accurate\"}\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"What types of prompts does SAM 2 accept?\"},\n",
    "    \"outputs\": {\"answer\": \"Points, boxes, and masks on individual frames\"}\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"What is the key architectural innovation for video processing?\"},\n",
    "    \"outputs\": {\"answer\": \"Transformer with streaming memory and memory bank\"}\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"How many videos are in the SA-V dataset?\"},\n",
    "    \"outputs\": {\"answer\": \"50.9K videos with 642.6K masklets\"}\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"What is SAM 2's performance on DAVIS 2017 val set?\"},\n",
    "    \"outputs\": {\"answer\": \"90.9 ð’¥&â„± with Hiera-B+ encoder\"}\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"How does SAM 2 handle multiple objects in video?\"},\n",
    "    \"outputs\": {\"answer\": \"Processes each object separately without inter-object communication\"}\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"What's the main limitation mentioned?\"},\n",
    "    \"outputs\": {\"answer\": \"Difficulty with shot changes and crowded scenes\"}\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"What future improvement is suggested?\"},\n",
    "    \"outputs\": {\"answer\": \"Incorporating explicit motion modeling\"}\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"question\": \"What's the CO2 emission equivalent of training?\"},\n",
    "    \"outputs\": {\"answer\": \"~3.89 metric tons (equivalent to 10k miles driven)\"}\n",
    "  }\n",
    "]\n",
    "\n",
    "# Create the dataset and examples in LangSmith\n",
    "dataset_name = \"SAM2 Q&A\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator\n",
    "### Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# Grade output schema\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    # Note that the order in the fields are defined is the order in which the model will generate them.\n",
    "    # It is useful to put explanations before responses because it forces the model to think through\n",
    "    # its final response before generating it:\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]\n",
    "\n",
    "# Grade prompt\n",
    "correctness_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
    "(2) Ensure that the student answer does not contain any conflicting statements.\n",
    "(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
    "\n",
    "Correctness:\n",
    "A correctness value of True means that the student's answer meets all of the criteria.\n",
    "A correctness value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "grader_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(CorrectnessGrade, method=\"json_schema\", strict=True)\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for RAG answer accuracy\"\"\"\n",
    "    answers = f\"\"\"\\\n",
    "    QUESTION: {inputs['question']}\n",
    "    GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
    "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
    "\n",
    "    # Run evaluator\n",
    "    grade = grader_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": correctness_instructions}, \n",
    "        {\"role\": \"user\", \"content\": answers}\n",
    "    ])\n",
    "    return grade[\"correct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-correctedness-7c3179a6' at:\n",
      "https://smith.langchain.com/o/61d90813-dbd8-5ea4-8a7a-14aa80a5a455/datasets/ab8ab590-4128-4f5d-8a8d-31b1097c313e/compare?selectedSessions=f904ed95-5a0e-4bc9-ac3e-dd22fa8e93d0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e15996a5054158b83b4167feddc1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def target(inputs: dict) -> dict:\n",
    "    return rag_bot(inputs[\"question\"])\n",
    "\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness],\n",
    "    experiment_prefix=\"rag-correctedness\",\n",
    "    metadata={\"version\": \"LCEL context, gpt-4o\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
