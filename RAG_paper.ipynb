{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 논문 질의응답 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma bs4 langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 논문 삽입과 질의응답 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../project/example_data/2408.00714v1.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG로 질의응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me some key points of the paper.',\n",
       " 'context': [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:')],\n",
       " 'answer': 'The paper discusses key ideas and demonstrations related to building LLM-centered agents. It highlights common limitations encountered in the process of developing such agents. The challenges include issues such as model biases, data limitations, and the need for interpretability and explainability in LLM-based systems.'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"Give me some key points of the paper.\"})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper discusses key ideas and demonstrations related to building LLM-centered agents. It highlights common limitations encountered in the process of developing such agents. The challenges include issues such as model biases, data limitations, and the need for interpretability and explainability in LLM-based systems.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 결과: 맥락을 이해하지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the paperbot. If you want to quit, please enter 'exit'.\n",
      "User: What is annotation?\n",
      "Assitant: Annotation is the process of adding metadata or labels to data, such as images, videos, or text, to provide additional context or information. It helps in categorizing, organizing, and making the data more understandable for machines or humans. Annotations can include things like bounding boxes around objects, text descriptions, or classifications that can be used for training machine learning models.\n",
      "User: What are common ways of doing it?\n",
      "Assitant: Building LLM-centered agents commonly involves challenges such as lack of interpretability in generated text, difficulty in fine-tuning large language models, and potential biases in the training data. These limitations can impact the performance and reliability of the agents in real-world applications.\n",
      "Thank you.\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to the paperbot. If you want to quit, please enter 'exit'.\")\n",
    "while True:\n",
    "    # Input\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # 종료 입력하면 대화 종료\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Thank you.\")\n",
    "        break\n",
    "\n",
    "    # 응답 생성 및 출력\n",
    "    results = rag_chain.invoke({\"input\": user_input})\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Assitant: {results['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대화형 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "### Construct retriever ###\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 결과: 대명사(it)를 이해함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the paperbot. If you want to quit, please enter 'exit'.\n",
      "User: What is annotation?\n",
      "Assitant: Annotation is the process of adding metadata or labels to data to provide additional information or context. In the context of data annotation, it involves labeling or marking data to make it understandable for machines, algorithms, or humans. Annotations help in training machine learning models, improving search algorithms, and enhancing data organization and retrieval.\n",
      "User: What are common ways of doing it?\n",
      "Assitant: Common ways of data annotation include manual annotation, where humans label data directly; automated annotation, using algorithms to label data automatically; semi-automated annotation, a combination of manual and automated methods; and collaborative annotation, involving multiple annotators working together to label data consistently. These methods are used in various fields like computer vision, natural language processing, and speech recognition to create labeled datasets for training machine learning models.\n",
      "User: In SAM2 What happens during a annotation process? Redo the search\n",
      "Assitant: I don't have that information.\n",
      "Thank you.\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to the paperbot. If you want to quit, please enter 'exit'.\")\n",
    "while True:\n",
    "    # Input\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # 종료 입력하면 대화 종료\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Thank you.\")\n",
    "        break\n",
    "\n",
    "    # 응답 생성 및 출력\n",
    "    results = conversational_rag_chain.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    "    )\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Assitant: {results['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "### Construct retriever ###\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "### Build retriever tool ###\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 결과: 맥락을 이해하고, 스스로 논문 안에서 관련 내용을 정리함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the paperbot. If you want to quit, please enter 'exit'.\n",
      "User: What is annotation?\n",
      "Assitant: Annotation is the process of adding metadata or labels to data to provide additional information or context. In the context of data annotation, annotators make subjective decisions to label or mark objects in videos, which can vary based on individual judgment. Annotators are assumed to be trained and understand the task, and in some cases, their contributions may not be validated or verified. There may be processes in place to communicate and distribute these annotated data to dataset consumers.\n",
      "User: What are common ways of doing it?\n",
      "Assitant: There are several common ways of performing data annotation, including:\n",
      "\n",
      "1. Manual Annotation: Human annotators manually label or tag data based on specific criteria or guidelines.\n",
      "\n",
      "2. Automated Annotation: Machine learning algorithms automatically annotate data based on predefined patterns or rules.\n",
      "\n",
      "3. Crowdsourced Annotation: Outsourcing annotation tasks to a large group of individuals through online platforms.\n",
      "\n",
      "4. Semi-Automated Annotation: Combining human expertise with automated tools to speed up the annotation process.\n",
      "\n",
      "5. Active Learning: Using machine learning models to select the most informative data points for annotation.\n",
      "\n",
      "6. Transfer Learning: Leveraging pre-annotated data or models to transfer knowledge and annotations to new datasets.\n",
      "\n",
      "These are some common methods used for data annotation, each with its own advantages and limitations depending on the specific requirements of the task.\n",
      "User: In SAM2 What happens during a annotation process? Redo the search\n",
      "Assitant: During the annotation process in SAM2, annotators focus on challenging objects where SAM2 struggles to improve the model's ability to \"segment anything.\" An online model in the loop setup is used, where annotators interactively use SAM2 to identify failure modes and correct them. The number of edited frames is used as a proxy for the challengingness of an object, and annotators are asked to annotate objects that require at least 2 edited frames with SAM2 in the loop. This approach helps to focus annotation on less prominent and more challenging cases in large-scale video segmentation annotation. SAM2 is deployed on GPU as an API and integrated into the annotation tool for interactive use.\n",
      "Thank you.\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to the paperbot. If you want to quit, please enter 'exit'.\")\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "while True:\n",
    "    # Input\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # 종료 입력하면 대화 종료\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Thank you.\")\n",
    "        break\n",
    "\n",
    "    # 응답 생성 및 출력\n",
    "\n",
    "    for s in agent_executor.stream(\n",
    "        {\"messages\": HumanMessage(content=user_input)}, config=config\n",
    "    ):\n",
    "        pass\n",
    "        # print(s)\n",
    "        \n",
    "\n",
    "    print(f\"User: {user_input}\")\n",
    "    s = s['agent']['messages'][0]\n",
    "    print(f\"Assitant: {parser.invoke(s)}\")\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
