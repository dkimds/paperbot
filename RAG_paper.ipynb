{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 논문 삽입과 질의응답 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../project/example_data/2408.00714v1.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG로 질의응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"Give me some key points of the paper.\"})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper acknowledges the contributions of various individuals involved in data support, annotation engineering, management, compute support, and project-level support. There is no plan to validate or verify further annotations for SA-V, but annotations were reviewed for quality assurance during training and production stages. Sociodemographic characteristics were not used to select annotators for masklet annotations, and diversity among crowdworkers was encouraged during video collection. The final labels in the dataset are a result of data cleaning and post-processing from individual annotator responses.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM앱으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the paperbot. If you want to quit, please enter 'exit'.\n",
      "User: \n",
      "Assitant: I'm sorry, but I can't see a question in your message. How can I assist you today?\n",
      "Thank you.\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to the paperbot. If you want to quit, please enter 'exit'.\")\n",
    "while True:\n",
    "    # Input\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # 종료 입력하면 대화 종료\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Thank you.\")\n",
    "        break\n",
    "\n",
    "    # 응답 생성 및 출력\n",
    "    results = rag_chain.invoke({\"input\": user_input})\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Assitant: {results['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서버 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m59339\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\n",
      " __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "|  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "|  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "|  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "|  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "|_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/chat/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /chat/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "\n",
      "\u001b[1;31;40mLANGSERVE:\u001b[0m ⚠️ Using pydantic 2.0.2. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details.\n",
      "\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60011 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[33m307 Temporary Redirect\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60011 - \"\u001b[1mGET /chat/playground HTTP/1.1\u001b[0m\" \u001b[33m307 Temporary Redirect\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60011 - \"\u001b[1mGET /chat/playground/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60011 - \"\u001b[1mGET /chat/playground/assets/index-86d4d9c0.js HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60012 - \"\u001b[1mGET /chat/playground/assets/index-434ff580.css HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60012 - \"\u001b[1mGET /chat/c/N4XyA/input_schema HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60011 - \"\u001b[1mGET /chat/c/N4XyA/output_schema HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:60013 - \"\u001b[1mPOST /chat/stream_log HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m59339\u001b[0m]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daeseong/Documents/GitHub/AI-master/project/./serve.py\", line 51, in <module>\n",
      "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/uvicorn/main.py\", line 577, in run\n",
      "    server.run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/asyncio/runners.py\", line 123, in run\n",
      "    raise KeyboardInterrupt()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./serve.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
